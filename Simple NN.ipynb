{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9d9c99d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install scikit-learn\n",
    "!pip install torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b895880",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.metrics import classification_report,accuracy_score,f1_score\n",
    "from sklearn.model_selection import GridSearchCV,cross_validate,train_test_split\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a8799fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = torchvision.transforms.Compose([torchvision.transforms.ToTensor(),\n",
    "                                              transforms.Lambda(lambda x: torch.flatten(x))])\n",
    "\n",
    "train_set=torchvision.datasets.FashionMNIST('./files/fashion-mnist/', train=True, download=True,\n",
    "                             transform=transformer)\n",
    "\n",
    "test_set=torchvision.datasets.FashionMNIST('./files/fashion-mnist/', train=False, download=True,\n",
    "                             transform=transformer)\n",
    "\n",
    "batch_size_train= len(train_set)//5\n",
    "batch_size_test=len(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "76f7e3eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "  train_set,\n",
    "  batch_size=batch_size_train, shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "  test_set,\n",
    "  batch_size=batch_size_test, shuffle=True)\n",
    "\n",
    "train_enumerated = enumerate(train_loader)\n",
    "batch_idx, (train_x, train_y) = next(train_enumerated)\n",
    "\n",
    "test_enumerated = enumerate(test_loader)\n",
    "batch_idx, (test_x, test_y) = next(test_enumerated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "54887a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_models(models):\n",
    "    results_short = {}\n",
    "    for score in scores:\n",
    "        print('='*40)\n",
    "        print(\"# Tuning hyper-parameters for %s\" % score)\n",
    "        print()\n",
    "\n",
    "        for m in model_lbls:\n",
    "            print('-'*40)\n",
    "            print(\"Trying model {}\".format(models[m]['name']))\n",
    "            clf = GridSearchCV(models[m]['estimator'], models[m]['param'], cv=5,\n",
    "                               scoring='%s_macro' % score, \n",
    "                               return_train_score = False,\n",
    "                               n_jobs = -1, \n",
    "                               )\n",
    "            clf.fit(train_x, train_y)\n",
    "            print_results(clf)\n",
    "            results_short[m] = clf.best_score_\n",
    "        print(\"Summary of results for {}\".format(score))\n",
    "        print(\"Estimator\")\n",
    "        for m in results_short.keys():\n",
    "            print(\"{}\\t - score: {:4.2}%\".format(models[m]['name'], results_short[m]))\n",
    "\n",
    "def print_results(model):\n",
    "    print(\"Best parameters set found on train set:\")\n",
    "    print()\n",
    "    # if best is linear there is no gamma parameter\n",
    "    print(model.best_params_)\n",
    "    print()\n",
    "    print(\"Grid scores on train set:\")\n",
    "    print()\n",
    "    means = model.cv_results_['mean_test_score']\n",
    "    stds = model.cv_results_['std_test_score']\n",
    "    params = model.cv_results_['params']\n",
    "    print(\"Mean test score: {}\".format(means))\n",
    "    print(\"Std test score: {}\".format(stds))\n",
    "    print(\"Params test score: {}\".format(params))\n",
    "    print()\n",
    "    print(\"Detailed classification report for the best parameter set:\")\n",
    "    print()\n",
    "    print(\"The model is trained on the full train set.\")\n",
    "    print(\"The scores are computed on the full test set.\")\n",
    "    print()\n",
    "    true_y, pred_y = test_y, model.predict(test_x)\n",
    "    print(classification_report(true_y, pred_y))\n",
    "    print()\n",
    "    \n",
    "def plot_scores_by_parameter(model,ks,X_train,X_test,y_train,y_test,visualize=True):\n",
    "    train_scores = []\n",
    "    test_scores = []\n",
    "    \n",
    "    for k in ks:\n",
    "        clf = model(k).fit(X_train, y_train)\n",
    "        train_score = clf.score(X_train, y_train)\n",
    "        test_score = clf.score(X_test, y_test)\n",
    "        \n",
    "        train_scores.append(train_score)\n",
    "        test_scores.append(test_score)\n",
    "    if visualize:\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(ks, train_scores, color='blue', label='train score')\n",
    "        plt.plot(ks, test_scores, color='green', label='test score')\n",
    "        plt.legend()\n",
    "    return train_scores,test_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "586d75b8",
   "metadata": {},
   "source": [
    "## Multi-layer Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2396f353",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a79287c5",
   "metadata": {},
   "source": [
    "### Single hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a11474e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "----------------------------------------\n",
      "Trying model Multi-layer Perceptron Classifier       \n",
      "Best parameters set found on train set:\n",
      "\n",
      "{'hidden_layer_sizes': 500}\n",
      "\n",
      "Grid scores on train set:\n",
      "\n",
      "Mean test score: [0.87074679 0.86400664 0.86118058 0.84918392]\n",
      "Std test score: [0.00538104 0.00347564 0.00586011 0.00547648]\n",
      "Params test score: [{'hidden_layer_sizes': 500}, {'hidden_layer_sizes': 200}, {'hidden_layer_sizes': 100}, {'hidden_layer_sizes': 50}]\n",
      "\n",
      "Detailed classification report for the best parameter set:\n",
      "\n",
      "The model is trained on the full train set.\n",
      "The scores are computed on the full test set.\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.81      0.83      1000\n",
      "           1       0.98      0.97      0.98      1000\n",
      "           2       0.77      0.79      0.78      1000\n",
      "           3       0.83      0.91      0.87      1000\n",
      "           4       0.81      0.76      0.79      1000\n",
      "           5       0.98      0.87      0.92      1000\n",
      "           6       0.69      0.66      0.68      1000\n",
      "           7       0.90      0.96      0.93      1000\n",
      "           8       0.94      0.97      0.95      1000\n",
      "           9       0.90      0.96      0.93      1000\n",
      "\n",
      "    accuracy                           0.87     10000\n",
      "   macro avg       0.87      0.87      0.87     10000\n",
      "weighted avg       0.87      0.87      0.87     10000\n",
      "\n",
      "\n",
      "Summary of results for f1\n",
      "Estimator\n",
      "Multi-layer Perceptron Classifier       \t - score: 0.87%\n"
     ]
    }
   ],
   "source": [
    "model_lbls = ['mp']\n",
    "\n",
    "models = {\n",
    "    'mp': {'name': 'Multi-layer Perceptron Classifier       ',\n",
    "           'estimator': MLPClassifier(random_state=0), \n",
    "           'param': [{'hidden_layer_sizes': [(500), (200), (100), (50)]}],\n",
    "          },\n",
    "}\n",
    "\n",
    "model_lbls = ['mp']\n",
    "scores = ['f1']\n",
    "\n",
    "evaluate_models(models)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c46fe92",
   "metadata": {},
   "source": [
    "We will try to find the best activation function for the hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "40bcb0a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "----------------------------------------\n",
      "Trying model Multi-layer Perceptron Classifier       \n",
      "Best parameters set found on train set:\n",
      "\n",
      "{'activation': 'relu'}\n",
      "\n",
      "Grid scores on train set:\n",
      "\n",
      "Mean test score: [0.81822396 0.86479294 0.86251006 0.87074679]\n",
      "Std test score: [0.00982759 0.00475728 0.00272013 0.00538104]\n",
      "Params test score: [{'activation': 'identity'}, {'activation': 'logistic'}, {'activation': 'tanh'}, {'activation': 'relu'}]\n",
      "\n",
      "Detailed classification report for the best parameter set:\n",
      "\n",
      "The model is trained on the full train set.\n",
      "The scores are computed on the full test set.\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.81      0.83      1000\n",
      "           1       0.98      0.97      0.98      1000\n",
      "           2       0.77      0.79      0.78      1000\n",
      "           3       0.83      0.91      0.87      1000\n",
      "           4       0.81      0.76      0.79      1000\n",
      "           5       0.98      0.87      0.92      1000\n",
      "           6       0.69      0.66      0.68      1000\n",
      "           7       0.90      0.96      0.93      1000\n",
      "           8       0.94      0.97      0.95      1000\n",
      "           9       0.90      0.96      0.93      1000\n",
      "\n",
      "    accuracy                           0.87     10000\n",
      "   macro avg       0.87      0.87      0.87     10000\n",
      "weighted avg       0.87      0.87      0.87     10000\n",
      "\n",
      "\n",
      "Summary of results for f1\n",
      "Estimator\n",
      "Multi-layer Perceptron Classifier       \t - score: 0.87%\n"
     ]
    }
   ],
   "source": [
    "model_lbls = ['mp']\n",
    "\n",
    "models = {\n",
    "    'mp': {'name': 'Multi-layer Perceptron Classifier       ',\n",
    "           'estimator': MLPClassifier(random_state=0, hidden_layer_sizes=(500)), \n",
    "           'param': [{'activation':['identity', 'logistic', 'tanh', 'relu']}],\n",
    "          },\n",
    "}\n",
    "\n",
    "model_lbls = ['mp']\n",
    "scores = ['f1']\n",
    "\n",
    "evaluate_models(models)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9607dff5",
   "metadata": {},
   "source": [
    "The default activation - rectified linear unit function seems to be optimal.\n",
    "Testing different regularization strength:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "35a0b1da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "----------------------------------------\n",
      "Trying model Multi-layer Perceptron Classifier       \n",
      "Iteration 1, loss = 0.76870678\n",
      "Iteration 2, loss = 0.48272789\n",
      "Iteration 3, loss = 0.43736476\n",
      "Iteration 4, loss = 0.39921654\n",
      "Iteration 5, loss = 0.36654638\n",
      "Iteration 6, loss = 0.35716572\n",
      "Iteration 7, loss = 0.33994561\n",
      "Iteration 8, loss = 0.32149088\n",
      "Iteration 9, loss = 0.30161304\n",
      "Iteration 10, loss = 0.31197904\n",
      "Iteration 11, loss = 0.27822737\n",
      "Iteration 12, loss = 0.26940289\n",
      "Iteration 13, loss = 0.26202919\n",
      "Iteration 14, loss = 0.24874819\n",
      "Iteration 15, loss = 0.24139113\n",
      "Iteration 16, loss = 0.24249691\n",
      "Iteration 17, loss = 0.21992352\n",
      "Iteration 18, loss = 0.21296530\n",
      "Iteration 19, loss = 0.20668017\n",
      "Iteration 20, loss = 0.20382384\n",
      "Iteration 21, loss = 0.18779216\n",
      "Iteration 22, loss = 0.19476406\n",
      "Iteration 23, loss = 0.17791156\n",
      "Iteration 24, loss = 0.17846975\n",
      "Iteration 25, loss = 0.17099973\n",
      "Iteration 26, loss = 0.16271505\n",
      "Iteration 27, loss = 0.15621008\n",
      "Iteration 28, loss = 0.14622534\n",
      "Iteration 29, loss = 0.14626470\n",
      "Iteration 30, loss = 0.14405311\n",
      "Iteration 31, loss = 0.13676599\n",
      "Iteration 32, loss = 0.13226750\n",
      "Iteration 33, loss = 0.13776588\n",
      "Iteration 34, loss = 0.13120016\n",
      "Iteration 35, loss = 0.12620698\n",
      "Iteration 36, loss = 0.11465414\n",
      "Iteration 37, loss = 0.10929532\n",
      "Iteration 38, loss = 0.10511950\n",
      "Iteration 39, loss = 0.11158012\n",
      "Iteration 40, loss = 0.10214818\n",
      "Iteration 41, loss = 0.09263790\n",
      "Iteration 42, loss = 0.09039202\n",
      "Iteration 43, loss = 0.09128731\n",
      "Iteration 44, loss = 0.09684475\n",
      "Iteration 45, loss = 0.09650712\n",
      "Iteration 46, loss = 0.08218147\n",
      "Iteration 47, loss = 0.07299973\n",
      "Iteration 48, loss = 0.07375946\n",
      "Iteration 49, loss = 0.06676882\n",
      "Iteration 50, loss = 0.08044785\n",
      "Iteration 51, loss = 0.07444740\n",
      "Iteration 52, loss = 0.06990942\n",
      "Iteration 53, loss = 0.06053531\n",
      "Iteration 54, loss = 0.06395812\n",
      "Iteration 55, loss = 0.05947673\n",
      "Iteration 56, loss = 0.06131741\n",
      "Iteration 57, loss = 0.05335990\n",
      "Iteration 58, loss = 0.05125039\n",
      "Iteration 59, loss = 0.05158767\n",
      "Iteration 60, loss = 0.04472800\n",
      "Iteration 61, loss = 0.05045744\n",
      "Iteration 62, loss = 0.05435992\n",
      "Iteration 63, loss = 0.05372841\n",
      "Iteration 64, loss = 0.04777547\n",
      "Iteration 65, loss = 0.04137477\n",
      "Iteration 66, loss = 0.04478951\n",
      "Iteration 67, loss = 0.03745984\n",
      "Iteration 68, loss = 0.03846465\n",
      "Iteration 69, loss = 0.03320995\n",
      "Iteration 70, loss = 0.03344910\n",
      "Iteration 71, loss = 0.03494423\n",
      "Iteration 72, loss = 0.03598999\n",
      "Iteration 73, loss = 0.04912503\n",
      "Iteration 74, loss = 0.04233003\n",
      "Iteration 75, loss = 0.03809930\n",
      "Iteration 76, loss = 0.02800822\n",
      "Iteration 77, loss = 0.02470608\n",
      "Iteration 78, loss = 0.02694008\n",
      "Iteration 79, loss = 0.02938267\n",
      "Iteration 80, loss = 0.02682013\n",
      "Iteration 81, loss = 0.05291898\n",
      "Iteration 82, loss = 0.05919173\n",
      "Iteration 83, loss = 0.03666948\n",
      "Iteration 84, loss = 0.03027098\n",
      "Iteration 85, loss = 0.02514053\n",
      "Iteration 86, loss = 0.02051872\n",
      "Iteration 87, loss = 0.02253543\n",
      "Iteration 88, loss = 0.02682413\n",
      "Iteration 89, loss = 0.02861056\n",
      "Iteration 90, loss = 0.02569102\n",
      "Iteration 91, loss = 0.01977444\n",
      "Iteration 92, loss = 0.01818696\n",
      "Iteration 93, loss = 0.01984228\n",
      "Iteration 94, loss = 0.02582712\n",
      "Iteration 95, loss = 0.07074027\n",
      "Iteration 96, loss = 0.06226350\n",
      "Iteration 97, loss = 0.03683417\n",
      "Iteration 98, loss = 0.02075218\n",
      "Iteration 99, loss = 0.01727120\n",
      "Iteration 100, loss = 0.01725168\n",
      "Iteration 101, loss = 0.01502787\n",
      "Iteration 102, loss = 0.01439643\n",
      "Iteration 103, loss = 0.01395682\n",
      "Iteration 104, loss = 0.01402632\n",
      "Iteration 105, loss = 0.01408451\n",
      "Iteration 106, loss = 0.01365477\n",
      "Iteration 107, loss = 0.01350201\n",
      "Iteration 108, loss = 0.01250528\n",
      "Iteration 109, loss = 0.01280950\n",
      "Iteration 110, loss = 0.01104243\n",
      "Iteration 111, loss = 0.01064413\n",
      "Iteration 112, loss = 0.01029359\n",
      "Iteration 113, loss = 0.01068334\n",
      "Iteration 114, loss = 0.01077193\n",
      "Iteration 115, loss = 0.02569421\n",
      "Iteration 116, loss = 0.04890272\n",
      "Iteration 117, loss = 0.05178688\n",
      "Iteration 118, loss = 0.07079846\n",
      "Iteration 119, loss = 0.05849975\n",
      "Iteration 120, loss = 0.02810276\n",
      "Iteration 121, loss = 0.03442094\n",
      "Iteration 122, loss = 0.02124477\n",
      "Iteration 123, loss = 0.01319120\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Best parameters set found on train set:\n",
      "\n",
      "{'alpha': 0.001}\n",
      "\n",
      "Grid scores on train set:\n",
      "\n",
      "Mean test score: [0.8717853  0.87074679 0.87371141 0.86768699]\n",
      "Std test score: [0.00682691 0.00538104 0.00396683 0.00710625]\n",
      "Params test score: [{'alpha': 1e-05}, {'alpha': 0.0001}, {'alpha': 0.001}, {'alpha': 0.01}]\n",
      "\n",
      "Detailed classification report for the best parameter set:\n",
      "\n",
      "The model is trained on the full train set.\n",
      "The scores are computed on the full test set.\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.83      0.83      1000\n",
      "           1       0.98      0.97      0.98      1000\n",
      "           2       0.77      0.78      0.78      1000\n",
      "           3       0.88      0.88      0.88      1000\n",
      "           4       0.77      0.82      0.80      1000\n",
      "           5       0.96      0.93      0.94      1000\n",
      "           6       0.71      0.65      0.68      1000\n",
      "           7       0.92      0.94      0.93      1000\n",
      "           8       0.95      0.96      0.95      1000\n",
      "           9       0.94      0.95      0.95      1000\n",
      "\n",
      "    accuracy                           0.87     10000\n",
      "   macro avg       0.87      0.87      0.87     10000\n",
      "weighted avg       0.87      0.87      0.87     10000\n",
      "\n",
      "\n",
      "Summary of results for f1\n",
      "Estimator\n",
      "Multi-layer Perceptron Classifier       \t - score: 0.87%\n"
     ]
    }
   ],
   "source": [
    "model_lbls = ['mp']\n",
    "\n",
    "models = {\n",
    "    'mp': {'name': 'Multi-layer Perceptron Classifier       ',\n",
    "           'estimator': MLPClassifier(random_state=0, hidden_layer_sizes=(500), verbose=False), \n",
    "           'param': [{'alpha':[0.00001, 0.0001, 0.001, 0.01]}],\n",
    "          },\n",
    "}\n",
    "\n",
    "model_lbls = ['mp']\n",
    "scores = ['f1']\n",
    "\n",
    "evaluate_models(models)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff3e222a",
   "metadata": {},
   "source": [
    "Alpha = 0.001 seems to be the best.\n",
    "More precise search:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d283e701",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "----------------------------------------\n",
      "Trying model Multi-layer Perceptron Classifier       \n",
      "Best parameters set found on train set:\n",
      "\n",
      "{'alpha': 0.0005}\n",
      "\n",
      "Grid scores on train set:\n",
      "\n",
      "Mean test score: [0.87273174 0.87216315 0.87101454 0.87268277]\n",
      "Std test score: [0.00468255 0.00557654 0.00174476 0.00579765]\n",
      "Params test score: [{'alpha': 0.0005}, {'alpha': 0.0007}, {'alpha': 0.002}, {'alpha': 0.003}]\n",
      "\n",
      "Detailed classification report for the best parameter set:\n",
      "\n",
      "The model is trained on the full train set.\n",
      "The scores are computed on the full test set.\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.83      0.82      1000\n",
      "           1       0.98      0.97      0.98      1000\n",
      "           2       0.77      0.79      0.78      1000\n",
      "           3       0.89      0.87      0.88      1000\n",
      "           4       0.78      0.80      0.79      1000\n",
      "           5       0.95      0.93      0.94      1000\n",
      "           6       0.69      0.67      0.68      1000\n",
      "           7       0.92      0.93      0.93      1000\n",
      "           8       0.95      0.96      0.96      1000\n",
      "           9       0.93      0.94      0.94      1000\n",
      "\n",
      "    accuracy                           0.87     10000\n",
      "   macro avg       0.87      0.87      0.87     10000\n",
      "weighted avg       0.87      0.87      0.87     10000\n",
      "\n",
      "\n",
      "Summary of results for f1\n",
      "Estimator\n",
      "Multi-layer Perceptron Classifier       \t - score: 0.87%\n"
     ]
    }
   ],
   "source": [
    "model_lbls = ['mp']\n",
    "\n",
    "models = {\n",
    "    'mp': {'name': 'Multi-layer Perceptron Classifier       ',\n",
    "           'estimator': MLPClassifier(random_state=0, hidden_layer_sizes=(500)), \n",
    "           'param': [{'alpha':[0.0005, 0.0007, 0.002, 0.003]}],\n",
    "          },\n",
    "}\n",
    "\n",
    "model_lbls = ['mp']\n",
    "scores = ['f1']\n",
    "\n",
    "evaluate_models(models)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e9a7550",
   "metadata": {},
   "source": [
    "The more detailed search with alpha close to 0.001 gave worse results.We will stick with alpha = 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56cf6152",
   "metadata": {},
   "source": [
    "### Testing on all the train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d156758b",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = torchvision.transforms.Compose([torchvision.transforms.ToTensor(),\n",
    "                                              transforms.Lambda(lambda x: torch.flatten(x))])\n",
    "\n",
    "train_set=torchvision.datasets.FashionMNIST('./files/fashion-mnist/', train=True, download=True,\n",
    "                             transform=transformer)\n",
    "\n",
    "test_set=torchvision.datasets.FashionMNIST('./files/fashion-mnist/', train=False, download=True,\n",
    "                             transform=transformer)\n",
    "\n",
    "batch_size_train= len(train_set)\n",
    "batch_size_test=len(test_set)\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "  train_set,\n",
    "  batch_size=batch_size_train, shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "  test_set,\n",
    "  batch_size=batch_size_test, shuffle=True)\n",
    "\n",
    "train_enumerated = enumerate(train_loader)\n",
    "batch_idx, (train_x, train_y) = next(train_enumerated)\n",
    "\n",
    "test_enumerated = enumerate(test_loader)\n",
    "batch_idx, (test_x, test_y) = next(test_enumerated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "25ead2cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.82      0.84      1000\n",
      "           1       0.98      0.98      0.98      1000\n",
      "           2       0.82      0.82      0.82      1000\n",
      "           3       0.91      0.90      0.90      1000\n",
      "           4       0.82      0.84      0.83      1000\n",
      "           5       0.98      0.97      0.98      1000\n",
      "           6       0.74      0.74      0.74      1000\n",
      "           7       0.95      0.97      0.96      1000\n",
      "           8       0.97      0.97      0.97      1000\n",
      "           9       0.96      0.96      0.96      1000\n",
      "\n",
      "    accuracy                           0.90     10000\n",
      "   macro avg       0.90      0.90      0.90     10000\n",
      "weighted avg       0.90      0.90      0.90     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mlp = MLPClassifier(random_state=0, alpha=0.001, hidden_layer_sizes=(500))\n",
    "\n",
    "mlp.fit(train_x, train_y)\n",
    "true_y, pred_y = test_y, mlp.predict(test_x)\n",
    "print(classification_report(true_y, pred_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd220a61",
   "metadata": {},
   "source": [
    "## Second hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4f2ef704",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "----------------------------------------\n",
      "Trying model Multi-layer Perceptron Classifier       \n",
      "Best parameters set found on train set:\n",
      "\n",
      "{'hidden_layer_sizes': (500, 300)}\n",
      "\n",
      "Grid scores on train set:\n",
      "\n",
      "Mean test score: [0.8736102  0.87558772 0.86897231 0.87256248]\n",
      "Std test score: [0.00683737 0.01032117 0.00216618 0.00868897]\n",
      "Params test score: [{'hidden_layer_sizes': (500, 500)}, {'hidden_layer_sizes': (500, 300)}, {'hidden_layer_sizes': (500, 200)}, {'hidden_layer_sizes': (500, 100)}]\n",
      "\n",
      "Detailed classification report for the best parameter set:\n",
      "\n",
      "The model is trained on the full train set.\n",
      "The scores are computed on the full test set.\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.82      0.80      1000\n",
      "           1       0.97      0.97      0.97      1000\n",
      "           2       0.73      0.82      0.77      1000\n",
      "           3       0.87      0.87      0.87      1000\n",
      "           4       0.78      0.78      0.78      1000\n",
      "           5       0.97      0.92      0.95      1000\n",
      "           6       0.73      0.61      0.66      1000\n",
      "           7       0.92      0.96      0.94      1000\n",
      "           8       0.97      0.96      0.96      1000\n",
      "           9       0.94      0.96      0.95      1000\n",
      "\n",
      "    accuracy                           0.87     10000\n",
      "   macro avg       0.87      0.87      0.87     10000\n",
      "weighted avg       0.87      0.87      0.87     10000\n",
      "\n",
      "\n",
      "Summary of results for f1\n",
      "Estimator\n",
      "Multi-layer Perceptron Classifier       \t - score: 0.88%\n"
     ]
    }
   ],
   "source": [
    "model_lbls = ['mp']\n",
    "\n",
    "models = {\n",
    "    'mp': {'name': 'Multi-layer Perceptron Classifier       ',\n",
    "           'estimator': MLPClassifier(random_state=0, alpha=0.001), \n",
    "           'param': [{'hidden_layer_sizes':[(500, 500), (500, 300), (500, 200), (500, 100)]}],\n",
    "          },\n",
    "}\n",
    "0\n",
    "model_lbls = ['mp']\n",
    "scores = ['f1']\n",
    "\n",
    "evaluate_models(models)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "831e27e6",
   "metadata": {},
   "source": [
    "Testing different combinations between neurons count:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "592ea251",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "----------------------------------------\n",
      "Trying model Multi-layer Perceptron Classifier       \n",
      "Best parameters set found on train set:\n",
      "\n",
      "{'hidden_layer_sizes': (500, 50)}\n",
      "\n",
      "Grid scores on train set:\n",
      "\n",
      "Mean test score: [0.86977518 0.8721577  0.87049862 0.8754242  0.87203675]\n",
      "Std test score: [0.0048808  0.00782659 0.00824288 0.00338346 0.00336529]\n",
      "Params test score: [{'hidden_layer_sizes': (600, 300)}, {'hidden_layer_sizes': (768, 200)}, {'hidden_layer_sizes': (400, 300)}, {'hidden_layer_sizes': (500, 50)}, {'hidden_layer_sizes': (300, 30)}]\n",
      "\n",
      "Detailed classification report for the best parameter set:\n",
      "\n",
      "The model is trained on the full train set.\n",
      "The scores are computed on the full test set.\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.81      0.81      1000\n",
      "           1       0.98      0.96      0.97      1000\n",
      "           2       0.74      0.79      0.77      1000\n",
      "           3       0.86      0.87      0.87      1000\n",
      "           4       0.78      0.76      0.77      1000\n",
      "           5       0.96      0.93      0.95      1000\n",
      "           6       0.69      0.66      0.67      1000\n",
      "           7       0.92      0.95      0.93      1000\n",
      "           8       0.95      0.96      0.96      1000\n",
      "           9       0.94      0.95      0.95      1000\n",
      "\n",
      "    accuracy                           0.86     10000\n",
      "   macro avg       0.86      0.86      0.86     10000\n",
      "weighted avg       0.86      0.86      0.86     10000\n",
      "\n",
      "\n",
      "Summary of results for f1\n",
      "Estimator\n",
      "Multi-layer Perceptron Classifier       \t - score: 0.88%\n"
     ]
    }
   ],
   "source": [
    "model_lbls = ['mp']\n",
    "\n",
    "models = {\n",
    "    'mp': {'name': 'Multi-layer Perceptron Classifier       ',\n",
    "           'estimator': MLPClassifier(random_state=0, alpha=0.001), \n",
    "           'param': [{'hidden_layer_sizes':[(600, 300), (768, 200), (400, 300), (500, 50), (300, 30)]}],\n",
    "          },\n",
    "}\n",
    "\n",
    "model_lbls = ['mp']\n",
    "scores = ['f1']\n",
    "\n",
    "evaluate_models(models)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebd074e7",
   "metadata": {},
   "source": [
    "It looks like 500 for the first layer is optimal. We got simmilar results with second layer 300 and 50, but worse with second layer 200 or 100."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd34ee29",
   "metadata": {},
   "source": [
    "### All data test for (500, 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cafed8f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = torchvision.transforms.Compose([torchvision.transforms.ToTensor(),\n",
    "                                              transforms.Lambda(lambda x: torch.flatten(x))])\n",
    "\n",
    "train_set=torchvision.datasets.FashionMNIST('./files/fashion-mnist/', train=True, download=True,\n",
    "                             transform=transformer)\n",
    "\n",
    "test_set=torchvision.datasets.FashionMNIST('./files/fashion-mnist/', train=False, download=True,\n",
    "                             transform=transformer)\n",
    "\n",
    "batch_size_train= len(train_set)\n",
    "batch_size_test=len(test_set)\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "  train_set,\n",
    "  batch_size=batch_size_train, shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "  test_set,\n",
    "  batch_size=batch_size_test, shuffle=True)\n",
    "\n",
    "train_enumerated = enumerate(train_loader)\n",
    "batch_idx, (train_x, train_y) = next(train_enumerated)\n",
    "\n",
    "test_enumerated = enumerate(test_loader)\n",
    "batch_idx, (test_x, test_y) = next(test_enumerated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9ac7106e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.81      0.84      1000\n",
      "           1       0.99      0.97      0.98      1000\n",
      "           2       0.83      0.83      0.83      1000\n",
      "           3       0.88      0.92      0.90      1000\n",
      "           4       0.81      0.84      0.82      1000\n",
      "           5       0.98      0.96      0.97      1000\n",
      "           6       0.73      0.73      0.73      1000\n",
      "           7       0.93      0.98      0.95      1000\n",
      "           8       0.98      0.97      0.97      1000\n",
      "           9       0.98      0.96      0.97      1000\n",
      "\n",
      "    accuracy                           0.90     10000\n",
      "   macro avg       0.90      0.90      0.90     10000\n",
      "weighted avg       0.90      0.90      0.90     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mlp = MLPClassifier(random_state=0, alpha=0.001, hidden_layer_sizes=(500, 300))\n",
    "\n",
    "mlp.fit(train_x, train_y)\n",
    "true_y, pred_y = test_y, mlp.predict(test_x)\n",
    "print(classification_report(true_y, pred_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51b672af",
   "metadata": {},
   "source": [
    "### Third hidden layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e126b105",
   "metadata": {},
   "source": [
    "It seems that we start to overfit because the train results are better than the test. We will try 500-300-50."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ed2f5f4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.78      0.79      1000\n",
      "           1       0.99      0.97      0.98      1000\n",
      "           2       0.77      0.77      0.77      1000\n",
      "           3       0.89      0.87      0.88      1000\n",
      "           4       0.79      0.77      0.78      1000\n",
      "           5       0.96      0.93      0.94      1000\n",
      "           6       0.63      0.72      0.67      1000\n",
      "           7       0.93      0.94      0.93      1000\n",
      "           8       0.98      0.94      0.96      1000\n",
      "           9       0.93      0.96      0.95      1000\n",
      "\n",
      "    accuracy                           0.86     10000\n",
      "   macro avg       0.87      0.86      0.87     10000\n",
      "weighted avg       0.87      0.86      0.87     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mlp = MLPClassifier(random_state=0, alpha=0.001, hidden_layer_sizes=(500, 300, 50))\n",
    "\n",
    "mlp.fit(train_x, train_y)\n",
    "true_y, pred_y = test_y, mlp.predict(test_x)\n",
    "print(classification_report(true_y, pred_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23c59305",
   "metadata": {},
   "source": [
    "Overall the model with the second layer starts to overfit and adding third layer does not improve the situation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
