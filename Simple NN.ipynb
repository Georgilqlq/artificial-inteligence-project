{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9d9c99d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install scikit-learn\n",
    "!pip install torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8b895880",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.metrics import classification_report,accuracy_score,f1_score\n",
    "from sklearn.model_selection import GridSearchCV,cross_validate,train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a8799fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = torchvision.transforms.Compose([torchvision.transforms.ToTensor(),\n",
    "                                              transforms.Lambda(lambda x: torch.flatten(x))])\n",
    "\n",
    "train_set=torchvision.datasets.FashionMNIST('./files/fashion-mnist/', train=True, download=True,\n",
    "                             transform=transformer)\n",
    "\n",
    "test_set=torchvision.datasets.FashionMNIST('./files/fashion-mnist/', train=False, download=True,\n",
    "                             transform=transformer)\n",
    "\n",
    "batch_size_train= len(train_set)//5\n",
    "batch_size_test=len(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "76f7e3eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "  train_set,\n",
    "  batch_size=batch_size_train, shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "  test_set,\n",
    "  batch_size=batch_size_test, shuffle=True)\n",
    "\n",
    "train_enumerated = enumerate(train_loader)\n",
    "batch_idx, (train_x, train_y) = next(train_enumerated)\n",
    "\n",
    "test_enumerated = enumerate(test_loader)\n",
    "batch_idx, (test_x, test_y) = next(test_enumerated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "54887a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_models(models):\n",
    "    results_short = {}\n",
    "    for score in scores:\n",
    "        print('='*40)\n",
    "        print(\"# Tuning hyper-parameters for %s\" % score)\n",
    "        print()\n",
    "\n",
    "        for m in model_lbls:\n",
    "            print('-'*40)\n",
    "            print(\"Trying model {}\".format(models[m]['name']))\n",
    "            clf = GridSearchCV(models[m]['estimator'], models[m]['param'], cv=5,\n",
    "                               scoring='%s_macro' % score, \n",
    "                               return_train_score = False,\n",
    "                               n_jobs = -1, \n",
    "                               )\n",
    "            clf.fit(train_x, train_y)\n",
    "            print_results(clf)\n",
    "            results_short[m] = clf.best_score_\n",
    "        print(\"Summary of results for {}\".format(score))\n",
    "        print(\"Estimator\")\n",
    "        for m in results_short.keys():\n",
    "            print(\"{}\\t - score: {:4.2}%\".format(models[m]['name'], results_short[m]))\n",
    "\n",
    "def print_results(model):\n",
    "    print(\"Best parameters set found on train set:\")\n",
    "    print()\n",
    "    # if best is linear there is no gamma parameter\n",
    "    print(model.best_params_)\n",
    "    print()\n",
    "    print(\"Grid scores on train set:\")\n",
    "    print()\n",
    "    means = model.cv_results_['mean_test_score']\n",
    "    stds = model.cv_results_['std_test_score']\n",
    "    params = model.cv_results_['params']\n",
    "    print(\"Mean test score: {}\".format(means))\n",
    "    print(\"Std test score: {}\".format(stds))\n",
    "    print(\"Params test score: {}\".format(params))\n",
    "    print()\n",
    "    print(\"Detailed classification report for the best parameter set:\")\n",
    "    print()\n",
    "    print(\"The model is trained on the full train set.\")\n",
    "    print(\"The scores are computed on the full test set.\")\n",
    "    print()\n",
    "    true_y, pred_y = test_y, model.predict(test_x)\n",
    "    print(classification_report(true_y, pred_y))\n",
    "    print()\n",
    "    \n",
    "def plot_scores_by_parameter(model,ks,X_train,X_test,y_train,y_test,visualize=True):\n",
    "    train_scores = []\n",
    "    test_scores = []\n",
    "    \n",
    "    for k in ks:\n",
    "        clf = model(k).fit(X_train, y_train)\n",
    "        train_score = clf.score(X_train, y_train)\n",
    "        test_score = clf.score(X_test, y_test)\n",
    "        \n",
    "        train_scores.append(train_score)\n",
    "        test_scores.append(test_score)\n",
    "    if visualize:\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(ks, train_scores, color='blue', label='train score')\n",
    "        plt.plot(ks, test_scores, color='green', label='test score')\n",
    "        plt.legend()\n",
    "    return train_scores,test_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "586d75b8",
   "metadata": {},
   "source": [
    "## Multi-layer Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2396f353",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a79287c5",
   "metadata": {},
   "source": [
    "### Single hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a11474e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "----------------------------------------\n",
      "Trying model Multi-layer Perceptron Classifier       \n",
      "Best parameters set found on train set:\n",
      "\n",
      "{'hidden_layer_sizes': 500}\n",
      "\n",
      "Grid scores on train set:\n",
      "\n",
      "Mean test score: [0.87074679 0.86400664 0.86118058 0.84918392]\n",
      "Std test score: [0.00538104 0.00347564 0.00586011 0.00547648]\n",
      "Params test score: [{'hidden_layer_sizes': 500}, {'hidden_layer_sizes': 200}, {'hidden_layer_sizes': 100}, {'hidden_layer_sizes': 50}]\n",
      "\n",
      "Detailed classification report for the best parameter set:\n",
      "\n",
      "The model is trained on the full train set.\n",
      "The scores are computed on the full test set.\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.81      0.83      1000\n",
      "           1       0.98      0.97      0.98      1000\n",
      "           2       0.77      0.79      0.78      1000\n",
      "           3       0.83      0.91      0.87      1000\n",
      "           4       0.81      0.76      0.79      1000\n",
      "           5       0.98      0.87      0.92      1000\n",
      "           6       0.69      0.66      0.68      1000\n",
      "           7       0.90      0.96      0.93      1000\n",
      "           8       0.94      0.97      0.95      1000\n",
      "           9       0.90      0.96      0.93      1000\n",
      "\n",
      "    accuracy                           0.87     10000\n",
      "   macro avg       0.87      0.87      0.87     10000\n",
      "weighted avg       0.87      0.87      0.87     10000\n",
      "\n",
      "\n",
      "Summary of results for f1\n",
      "Estimator\n",
      "Multi-layer Perceptron Classifier       \t - score: 0.87%\n"
     ]
    }
   ],
   "source": [
    "model_lbls = ['mp']\n",
    "\n",
    "models = {\n",
    "    'mp': {'name': 'Multi-layer Perceptron Classifier       ',\n",
    "           'estimator': MLPClassifier(random_state=0), \n",
    "           'param': [{'hidden_layer_sizes': [(500), (200), (100), (50)]}],\n",
    "          },\n",
    "}\n",
    "\n",
    "model_lbls = ['mp']\n",
    "scores = ['f1']\n",
    "\n",
    "evaluate_models(models)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c46fe92",
   "metadata": {},
   "source": [
    "We will try to find the best activation function for the hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "40bcb0a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "----------------------------------------\n",
      "Trying model Multi-layer Perceptron Classifier       \n",
      "Best parameters set found on train set:\n",
      "\n",
      "{'activation': 'relu'}\n",
      "\n",
      "Grid scores on train set:\n",
      "\n",
      "Mean test score: [0.81822396 0.86479294 0.86251006 0.87074679]\n",
      "Std test score: [0.00982759 0.00475728 0.00272013 0.00538104]\n",
      "Params test score: [{'activation': 'identity'}, {'activation': 'logistic'}, {'activation': 'tanh'}, {'activation': 'relu'}]\n",
      "\n",
      "Detailed classification report for the best parameter set:\n",
      "\n",
      "The model is trained on the full train set.\n",
      "The scores are computed on the full test set.\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.81      0.83      1000\n",
      "           1       0.98      0.97      0.98      1000\n",
      "           2       0.77      0.79      0.78      1000\n",
      "           3       0.83      0.91      0.87      1000\n",
      "           4       0.81      0.76      0.79      1000\n",
      "           5       0.98      0.87      0.92      1000\n",
      "           6       0.69      0.66      0.68      1000\n",
      "           7       0.90      0.96      0.93      1000\n",
      "           8       0.94      0.97      0.95      1000\n",
      "           9       0.90      0.96      0.93      1000\n",
      "\n",
      "    accuracy                           0.87     10000\n",
      "   macro avg       0.87      0.87      0.87     10000\n",
      "weighted avg       0.87      0.87      0.87     10000\n",
      "\n",
      "\n",
      "Summary of results for f1\n",
      "Estimator\n",
      "Multi-layer Perceptron Classifier       \t - score: 0.87%\n"
     ]
    }
   ],
   "source": [
    "model_lbls = ['mp']\n",
    "\n",
    "models = {\n",
    "    'mp': {'name': 'Multi-layer Perceptron Classifier       ',\n",
    "           'estimator': MLPClassifier(random_state=0, hidden_layer_sizes=(500)), \n",
    "           'param': [{'activation':['identity', 'logistic', 'tanh', 'relu']}],\n",
    "          },\n",
    "}\n",
    "\n",
    "model_lbls = ['mp']\n",
    "scores = ['f1']\n",
    "\n",
    "evaluate_models(models)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9607dff5",
   "metadata": {},
   "source": [
    "The default activation - rectified linear unit function seems to be optimal.\n",
    "Testing different regularization strength:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "35a0b1da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "----------------------------------------\n",
      "Trying model Multi-layer Perceptron Classifier       \n",
      "Iteration 1, loss = 0.76870678\n",
      "Iteration 2, loss = 0.48272789\n",
      "Iteration 3, loss = 0.43736476\n",
      "Iteration 4, loss = 0.39921654\n",
      "Iteration 5, loss = 0.36654638\n",
      "Iteration 6, loss = 0.35716572\n",
      "Iteration 7, loss = 0.33994561\n",
      "Iteration 8, loss = 0.32149088\n",
      "Iteration 9, loss = 0.30161304\n",
      "Iteration 10, loss = 0.31197904\n",
      "Iteration 11, loss = 0.27822737\n",
      "Iteration 12, loss = 0.26940289\n",
      "Iteration 13, loss = 0.26202919\n",
      "Iteration 14, loss = 0.24874819\n",
      "Iteration 15, loss = 0.24139113\n",
      "Iteration 16, loss = 0.24249691\n",
      "Iteration 17, loss = 0.21992352\n",
      "Iteration 18, loss = 0.21296530\n",
      "Iteration 19, loss = 0.20668017\n",
      "Iteration 20, loss = 0.20382384\n",
      "Iteration 21, loss = 0.18779216\n",
      "Iteration 22, loss = 0.19476406\n",
      "Iteration 23, loss = 0.17791156\n",
      "Iteration 24, loss = 0.17846975\n",
      "Iteration 25, loss = 0.17099973\n",
      "Iteration 26, loss = 0.16271505\n",
      "Iteration 27, loss = 0.15621008\n",
      "Iteration 28, loss = 0.14622534\n",
      "Iteration 29, loss = 0.14626470\n",
      "Iteration 30, loss = 0.14405311\n",
      "Iteration 31, loss = 0.13676599\n",
      "Iteration 32, loss = 0.13226750\n",
      "Iteration 33, loss = 0.13776588\n",
      "Iteration 34, loss = 0.13120016\n",
      "Iteration 35, loss = 0.12620698\n",
      "Iteration 36, loss = 0.11465414\n",
      "Iteration 37, loss = 0.10929532\n",
      "Iteration 38, loss = 0.10511950\n",
      "Iteration 39, loss = 0.11158012\n",
      "Iteration 40, loss = 0.10214818\n",
      "Iteration 41, loss = 0.09263790\n",
      "Iteration 42, loss = 0.09039202\n",
      "Iteration 43, loss = 0.09128731\n",
      "Iteration 44, loss = 0.09684475\n",
      "Iteration 45, loss = 0.09650712\n",
      "Iteration 46, loss = 0.08218147\n",
      "Iteration 47, loss = 0.07299973\n",
      "Iteration 48, loss = 0.07375946\n",
      "Iteration 49, loss = 0.06676882\n",
      "Iteration 50, loss = 0.08044785\n",
      "Iteration 51, loss = 0.07444740\n",
      "Iteration 52, loss = 0.06990942\n",
      "Iteration 53, loss = 0.06053531\n",
      "Iteration 54, loss = 0.06395812\n",
      "Iteration 55, loss = 0.05947673\n",
      "Iteration 56, loss = 0.06131741\n",
      "Iteration 57, loss = 0.05335990\n",
      "Iteration 58, loss = 0.05125039\n",
      "Iteration 59, loss = 0.05158767\n",
      "Iteration 60, loss = 0.04472800\n",
      "Iteration 61, loss = 0.05045744\n",
      "Iteration 62, loss = 0.05435992\n",
      "Iteration 63, loss = 0.05372841\n",
      "Iteration 64, loss = 0.04777547\n",
      "Iteration 65, loss = 0.04137477\n",
      "Iteration 66, loss = 0.04478951\n",
      "Iteration 67, loss = 0.03745984\n",
      "Iteration 68, loss = 0.03846465\n",
      "Iteration 69, loss = 0.03320995\n",
      "Iteration 70, loss = 0.03344910\n",
      "Iteration 71, loss = 0.03494423\n",
      "Iteration 72, loss = 0.03598999\n",
      "Iteration 73, loss = 0.04912503\n",
      "Iteration 74, loss = 0.04233003\n",
      "Iteration 75, loss = 0.03809930\n",
      "Iteration 76, loss = 0.02800822\n",
      "Iteration 77, loss = 0.02470608\n",
      "Iteration 78, loss = 0.02694008\n",
      "Iteration 79, loss = 0.02938267\n",
      "Iteration 80, loss = 0.02682013\n",
      "Iteration 81, loss = 0.05291898\n",
      "Iteration 82, loss = 0.05919173\n",
      "Iteration 83, loss = 0.03666948\n",
      "Iteration 84, loss = 0.03027098\n",
      "Iteration 85, loss = 0.02514053\n",
      "Iteration 86, loss = 0.02051872\n",
      "Iteration 87, loss = 0.02253543\n",
      "Iteration 88, loss = 0.02682413\n",
      "Iteration 89, loss = 0.02861056\n",
      "Iteration 90, loss = 0.02569102\n",
      "Iteration 91, loss = 0.01977444\n",
      "Iteration 92, loss = 0.01818696\n",
      "Iteration 93, loss = 0.01984228\n",
      "Iteration 94, loss = 0.02582712\n",
      "Iteration 95, loss = 0.07074027\n",
      "Iteration 96, loss = 0.06226350\n",
      "Iteration 97, loss = 0.03683417\n",
      "Iteration 98, loss = 0.02075218\n",
      "Iteration 99, loss = 0.01727120\n",
      "Iteration 100, loss = 0.01725168\n",
      "Iteration 101, loss = 0.01502787\n",
      "Iteration 102, loss = 0.01439643\n",
      "Iteration 103, loss = 0.01395682\n",
      "Iteration 104, loss = 0.01402632\n",
      "Iteration 105, loss = 0.01408451\n",
      "Iteration 106, loss = 0.01365477\n",
      "Iteration 107, loss = 0.01350201\n",
      "Iteration 108, loss = 0.01250528\n",
      "Iteration 109, loss = 0.01280950\n",
      "Iteration 110, loss = 0.01104243\n",
      "Iteration 111, loss = 0.01064413\n",
      "Iteration 112, loss = 0.01029359\n",
      "Iteration 113, loss = 0.01068334\n",
      "Iteration 114, loss = 0.01077193\n",
      "Iteration 115, loss = 0.02569421\n",
      "Iteration 116, loss = 0.04890272\n",
      "Iteration 117, loss = 0.05178688\n",
      "Iteration 118, loss = 0.07079846\n",
      "Iteration 119, loss = 0.05849975\n",
      "Iteration 120, loss = 0.02810276\n",
      "Iteration 121, loss = 0.03442094\n",
      "Iteration 122, loss = 0.02124477\n",
      "Iteration 123, loss = 0.01319120\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Best parameters set found on train set:\n",
      "\n",
      "{'alpha': 0.001}\n",
      "\n",
      "Grid scores on train set:\n",
      "\n",
      "Mean test score: [0.8717853  0.87074679 0.87371141 0.86768699]\n",
      "Std test score: [0.00682691 0.00538104 0.00396683 0.00710625]\n",
      "Params test score: [{'alpha': 1e-05}, {'alpha': 0.0001}, {'alpha': 0.001}, {'alpha': 0.01}]\n",
      "\n",
      "Detailed classification report for the best parameter set:\n",
      "\n",
      "The model is trained on the full train set.\n",
      "The scores are computed on the full test set.\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.83      0.83      1000\n",
      "           1       0.98      0.97      0.98      1000\n",
      "           2       0.77      0.78      0.78      1000\n",
      "           3       0.88      0.88      0.88      1000\n",
      "           4       0.77      0.82      0.80      1000\n",
      "           5       0.96      0.93      0.94      1000\n",
      "           6       0.71      0.65      0.68      1000\n",
      "           7       0.92      0.94      0.93      1000\n",
      "           8       0.95      0.96      0.95      1000\n",
      "           9       0.94      0.95      0.95      1000\n",
      "\n",
      "    accuracy                           0.87     10000\n",
      "   macro avg       0.87      0.87      0.87     10000\n",
      "weighted avg       0.87      0.87      0.87     10000\n",
      "\n",
      "\n",
      "Summary of results for f1\n",
      "Estimator\n",
      "Multi-layer Perceptron Classifier       \t - score: 0.87%\n"
     ]
    }
   ],
   "source": [
    "model_lbls = ['mp']\n",
    "\n",
    "models = {\n",
    "    'mp': {'name': 'Multi-layer Perceptron Classifier       ',\n",
    "           'estimator': MLPClassifier(random_state=0, hidden_layer_sizes=(500), verbose=False), \n",
    "           'param': [{'alpha':[0.00001, 0.0001, 0.001, 0.01]}],\n",
    "          },\n",
    "}\n",
    "\n",
    "model_lbls = ['mp']\n",
    "scores = ['f1']\n",
    "\n",
    "evaluate_models(models)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff3e222a",
   "metadata": {},
   "source": [
    "Alpha = 0.001 seems to be the best.\n",
    "More precise search:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d283e701",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "----------------------------------------\n",
      "Trying model Multi-layer Perceptron Classifier       \n",
      "Best parameters set found on train set:\n",
      "\n",
      "{'alpha': 0.0005}\n",
      "\n",
      "Grid scores on train set:\n",
      "\n",
      "Mean test score: [0.87273174 0.87216315 0.87101454 0.87268277]\n",
      "Std test score: [0.00468255 0.00557654 0.00174476 0.00579765]\n",
      "Params test score: [{'alpha': 0.0005}, {'alpha': 0.0007}, {'alpha': 0.002}, {'alpha': 0.003}]\n",
      "\n",
      "Detailed classification report for the best parameter set:\n",
      "\n",
      "The model is trained on the full train set.\n",
      "The scores are computed on the full test set.\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.83      0.82      1000\n",
      "           1       0.98      0.97      0.98      1000\n",
      "           2       0.77      0.79      0.78      1000\n",
      "           3       0.89      0.87      0.88      1000\n",
      "           4       0.78      0.80      0.79      1000\n",
      "           5       0.95      0.93      0.94      1000\n",
      "           6       0.69      0.67      0.68      1000\n",
      "           7       0.92      0.93      0.93      1000\n",
      "           8       0.95      0.96      0.96      1000\n",
      "           9       0.93      0.94      0.94      1000\n",
      "\n",
      "    accuracy                           0.87     10000\n",
      "   macro avg       0.87      0.87      0.87     10000\n",
      "weighted avg       0.87      0.87      0.87     10000\n",
      "\n",
      "\n",
      "Summary of results for f1\n",
      "Estimator\n",
      "Multi-layer Perceptron Classifier       \t - score: 0.87%\n"
     ]
    }
   ],
   "source": [
    "model_lbls = ['mp']\n",
    "\n",
    "models = {\n",
    "    'mp': {'name': 'Multi-layer Perceptron Classifier       ',\n",
    "           'estimator': MLPClassifier(random_state=0, hidden_layer_sizes=(500)), \n",
    "           'param': [{'alpha':[0.0005, 0.0007, 0.002, 0.003]}],\n",
    "          },\n",
    "}\n",
    "\n",
    "model_lbls = ['mp']\n",
    "scores = ['f1']\n",
    "\n",
    "evaluate_models(models)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e9a7550",
   "metadata": {},
   "source": [
    "The more detailed search with alpha close to 0.001 gave worse results.We will stick with alpha = 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56cf6152",
   "metadata": {},
   "source": [
    "### Testing on all the train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d156758b",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = torchvision.transforms.Compose([torchvision.transforms.ToTensor(),\n",
    "                                              transforms.Lambda(lambda x: torch.flatten(x))])\n",
    "\n",
    "train_set=torchvision.datasets.FashionMNIST('./files/fashion-mnist/', train=True, download=True,\n",
    "                             transform=transformer)\n",
    "\n",
    "test_set=torchvision.datasets.FashionMNIST('./files/fashion-mnist/', train=False, download=True,\n",
    "                             transform=transformer)\n",
    "\n",
    "batch_size_train= len(train_set)\n",
    "batch_size_test=len(test_set)\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "  train_set,\n",
    "  batch_size=batch_size_train, shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "  test_set,\n",
    "  batch_size=batch_size_test, shuffle=True)\n",
    "\n",
    "train_enumerated = enumerate(train_loader)\n",
    "batch_idx, (train_x, train_y) = next(train_enumerated)\n",
    "\n",
    "test_enumerated = enumerate(test_loader)\n",
    "batch_idx, (test_x, test_y) = next(test_enumerated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "25ead2cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.82      0.84      1000\n",
      "           1       0.98      0.98      0.98      1000\n",
      "           2       0.82      0.82      0.82      1000\n",
      "           3       0.91      0.90      0.90      1000\n",
      "           4       0.82      0.84      0.83      1000\n",
      "           5       0.98      0.97      0.98      1000\n",
      "           6       0.74      0.74      0.74      1000\n",
      "           7       0.95      0.97      0.96      1000\n",
      "           8       0.97      0.97      0.97      1000\n",
      "           9       0.96      0.96      0.96      1000\n",
      "\n",
      "    accuracy                           0.90     10000\n",
      "   macro avg       0.90      0.90      0.90     10000\n",
      "weighted avg       0.90      0.90      0.90     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mlp = MLPClassifier(random_state=0, alpha=0.001, hidden_layer_sizes=(500))\n",
    "\n",
    "mlp.fit(train_x, train_y)\n",
    "true_y, pred_y = test_y, mlp.predict(test_x)\n",
    "print(classification_report(true_y, pred_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd220a61",
   "metadata": {},
   "source": [
    "## Second hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4f2ef704",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "----------------------------------------\n",
      "Trying model Multi-layer Perceptron Classifier       \n",
      "Best parameters set found on train set:\n",
      "\n",
      "{'hidden_layer_sizes': (500, 300)}\n",
      "\n",
      "Grid scores on train set:\n",
      "\n",
      "Mean test score: [0.8736102  0.87558772 0.86897231 0.87256248]\n",
      "Std test score: [0.00683737 0.01032117 0.00216618 0.00868897]\n",
      "Params test score: [{'hidden_layer_sizes': (500, 500)}, {'hidden_layer_sizes': (500, 300)}, {'hidden_layer_sizes': (500, 200)}, {'hidden_layer_sizes': (500, 100)}]\n",
      "\n",
      "Detailed classification report for the best parameter set:\n",
      "\n",
      "The model is trained on the full train set.\n",
      "The scores are computed on the full test set.\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.82      0.80      1000\n",
      "           1       0.97      0.97      0.97      1000\n",
      "           2       0.73      0.82      0.77      1000\n",
      "           3       0.87      0.87      0.87      1000\n",
      "           4       0.78      0.78      0.78      1000\n",
      "           5       0.97      0.92      0.95      1000\n",
      "           6       0.73      0.61      0.66      1000\n",
      "           7       0.92      0.96      0.94      1000\n",
      "           8       0.97      0.96      0.96      1000\n",
      "           9       0.94      0.96      0.95      1000\n",
      "\n",
      "    accuracy                           0.87     10000\n",
      "   macro avg       0.87      0.87      0.87     10000\n",
      "weighted avg       0.87      0.87      0.87     10000\n",
      "\n",
      "\n",
      "Summary of results for f1\n",
      "Estimator\n",
      "Multi-layer Perceptron Classifier       \t - score: 0.88%\n"
     ]
    }
   ],
   "source": [
    "model_lbls = ['mp']\n",
    "\n",
    "models = {\n",
    "    'mp': {'name': 'Multi-layer Perceptron Classifier       ',\n",
    "           'estimator': MLPClassifier(random_state=0, alpha=0.001), \n",
    "           'param': [{'hidden_layer_sizes':[(500, 500), (500, 300), (500, 200), (500, 100)]}],\n",
    "          },\n",
    "}\n",
    "0\n",
    "model_lbls = ['mp']\n",
    "scores = ['f1']\n",
    "\n",
    "evaluate_models(models)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "831e27e6",
   "metadata": {},
   "source": [
    "Testing different combinations between neurons count:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "592ea251",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "----------------------------------------\n",
      "Trying model Multi-layer Perceptron Classifier       \n",
      "Best parameters set found on train set:\n",
      "\n",
      "{'hidden_layer_sizes': (500, 50)}\n",
      "\n",
      "Grid scores on train set:\n",
      "\n",
      "Mean test score: [0.86977518 0.8721577  0.87049862 0.8754242  0.87203675]\n",
      "Std test score: [0.0048808  0.00782659 0.00824288 0.00338346 0.00336529]\n",
      "Params test score: [{'hidden_layer_sizes': (600, 300)}, {'hidden_layer_sizes': (768, 200)}, {'hidden_layer_sizes': (400, 300)}, {'hidden_layer_sizes': (500, 50)}, {'hidden_layer_sizes': (300, 30)}]\n",
      "\n",
      "Detailed classification report for the best parameter set:\n",
      "\n",
      "The model is trained on the full train set.\n",
      "The scores are computed on the full test set.\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.81      0.81      1000\n",
      "           1       0.98      0.96      0.97      1000\n",
      "           2       0.74      0.79      0.77      1000\n",
      "           3       0.86      0.87      0.87      1000\n",
      "           4       0.78      0.76      0.77      1000\n",
      "           5       0.96      0.93      0.95      1000\n",
      "           6       0.69      0.66      0.67      1000\n",
      "           7       0.92      0.95      0.93      1000\n",
      "           8       0.95      0.96      0.96      1000\n",
      "           9       0.94      0.95      0.95      1000\n",
      "\n",
      "    accuracy                           0.86     10000\n",
      "   macro avg       0.86      0.86      0.86     10000\n",
      "weighted avg       0.86      0.86      0.86     10000\n",
      "\n",
      "\n",
      "Summary of results for f1\n",
      "Estimator\n",
      "Multi-layer Perceptron Classifier       \t - score: 0.88%\n"
     ]
    }
   ],
   "source": [
    "model_lbls = ['mp']\n",
    "\n",
    "models = {\n",
    "    'mp': {'name': 'Multi-layer Perceptron Classifier       ',\n",
    "           'estimator': MLPClassifier(random_state=0, alpha=0.001), \n",
    "           'param': [{'hidden_layer_sizes':[(600, 300), (768, 200), (400, 300), (500, 50), (300, 30)]}],\n",
    "          },\n",
    "}\n",
    "\n",
    "model_lbls = ['mp']\n",
    "scores = ['f1']\n",
    "\n",
    "evaluate_models(models)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebd074e7",
   "metadata": {},
   "source": [
    "It looks like 500 for the first layer is optimal. We got simmilar results with second layer 300 and 50, but worse with second layer 200 or 100."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd34ee29",
   "metadata": {},
   "source": [
    "### All data test for (500, 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cafed8f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = torchvision.transforms.Compose([torchvision.transforms.ToTensor(),\n",
    "                                              transforms.Lambda(lambda x: torch.flatten(x))])\n",
    "\n",
    "train_set=torchvision.datasets.FashionMNIST('./files/fashion-mnist/', train=True, download=True,\n",
    "                             transform=transformer)\n",
    "\n",
    "test_set=torchvision.datasets.FashionMNIST('./files/fashion-mnist/', train=False, download=True,\n",
    "                             transform=transformer)\n",
    "\n",
    "batch_size_train= len(train_set)\n",
    "batch_size_test=len(test_set)\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "  train_set,\n",
    "  batch_size=batch_size_train, shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "  test_set,\n",
    "  batch_size=batch_size_test, shuffle=True)\n",
    "\n",
    "train_enumerated = enumerate(train_loader)\n",
    "batch_idx, (train_x, train_y) = next(train_enumerated)\n",
    "\n",
    "test_enumerated = enumerate(test_loader)\n",
    "batch_idx, (test_x, test_y) = next(test_enumerated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9ac7106e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.81      0.84      1000\n",
      "           1       0.99      0.97      0.98      1000\n",
      "           2       0.83      0.83      0.83      1000\n",
      "           3       0.88      0.92      0.90      1000\n",
      "           4       0.81      0.84      0.82      1000\n",
      "           5       0.98      0.96      0.97      1000\n",
      "           6       0.73      0.73      0.73      1000\n",
      "           7       0.93      0.98      0.95      1000\n",
      "           8       0.98      0.97      0.97      1000\n",
      "           9       0.98      0.96      0.97      1000\n",
      "\n",
      "    accuracy                           0.90     10000\n",
      "   macro avg       0.90      0.90      0.90     10000\n",
      "weighted avg       0.90      0.90      0.90     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mlp = MLPClassifier(random_state=0, alpha=0.001, hidden_layer_sizes=(500, 300))\n",
    "\n",
    "mlp.fit(train_x, train_y)\n",
    "true_y, pred_y = test_y, mlp.predict(test_x)\n",
    "print(classification_report(true_y, pred_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2933743b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid = train_test_split(train_x, train_y, random_state=0)\n",
    "\n",
    "model = MLPClassifier(random_state=0, alpha=0.001, hidden_layer_sizes=(500, 300)).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8a1c5b72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   MLP 500-300  Accuracy\n",
      "0  Train score  0.996889\n",
      "1   Test score  0.900933\n"
     ]
    }
   ],
   "source": [
    "train_score = model.score(X_train, y_train)\n",
    "test_score = model.score(X_valid, y_valid)\n",
    "\n",
    "data = {'MLP 500-300':  ['Train score', 'Test score'],\n",
    "        'Accuracy': [train_score ,test_score],\n",
    "        }\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cbc1aa76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1600x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "barplot = df.plot.bar(x='MLP 500-300', y='Accuracy', rot=0, figsize=(16,8))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51b672af",
   "metadata": {},
   "source": [
    "### Third hidden layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e126b105",
   "metadata": {},
   "source": [
    "It seems that we start to overfit because the train results are better than the test. We will try 500-300-50."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ed2f5f4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.78      0.79      1000\n",
      "           1       0.99      0.97      0.98      1000\n",
      "           2       0.77      0.77      0.77      1000\n",
      "           3       0.89      0.87      0.88      1000\n",
      "           4       0.79      0.77      0.78      1000\n",
      "           5       0.96      0.93      0.94      1000\n",
      "           6       0.63      0.72      0.67      1000\n",
      "           7       0.93      0.94      0.93      1000\n",
      "           8       0.98      0.94      0.96      1000\n",
      "           9       0.93      0.96      0.95      1000\n",
      "\n",
      "    accuracy                           0.86     10000\n",
      "   macro avg       0.87      0.86      0.87     10000\n",
      "weighted avg       0.87      0.86      0.87     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mlp = MLPClassifier(random_state=0, alpha=0.001, hidden_layer_sizes=(500, 300, 50))\n",
    "\n",
    "mlp.fit(train_x, train_y)\n",
    "true_y, pred_y = test_y, mlp.predict(test_x)\n",
    "print(classification_report(true_y, pred_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23c59305",
   "metadata": {},
   "source": [
    "Overall the model with the second layer starts to overfit and adding third layer does not improve the situation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
